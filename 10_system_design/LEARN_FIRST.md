# System Design for AI Engineers â€” Complete Guide

> **Read this first.** This is your definitive reference for designing production AI systems in interviews and on the job. Every architecture includes diagrams, technology choices, trade-offs, and real numbers.

---

## Table of Contents

1. [Why System Design Matters for AI Engineers](#1-why-system-design-matters)
2. [The AI System Design Framework](#2-the-ai-system-design-framework)
3. [Designing a Production RAG System](#3-designing-a-production-rag-system)
4. [Designing an LLM Application Platform](#4-designing-an-llm-application-platform)
5. [Designing ML Model Serving Infrastructure](#5-designing-ml-model-serving-infrastructure)
6. [Designing an MLOps Pipeline](#6-designing-an-mlops-pipeline)
7. [Production Considerations for AI Systems](#7-production-considerations)
8. [Common AI System Design Trade-offs](#8-common-trade-offs)
9. [AI System Design Interview Questions with Answers](#9-interview-questions-with-answers)

---

## 1. Why System Design Matters

AI engineering is no longer just about training models. In production:

- **80% of the work** is infrastructure, data pipelines, serving, and monitoring
- **Models are the easy part** â€” the hard part is reliability, cost, latency, and scale
- **Interviewers test** whether you can build systems that work at scale, not just notebooks

### What Interviewers Evaluate

| Dimension | What They Look For |
|-----------|-------------------|
| **Structured Thinking** | Do you clarify requirements before designing? |
| **Architecture** | Can you decompose a system into well-defined components? |
| **Trade-offs** | Do you explain WHY you chose X over Y? |
| **AI-Specific Knowledge** | Embeddings, retrieval, model serving, evaluation |
| **Production Readiness** | Monitoring, cost, failure handling, scaling |
| **Communication** | Can you explain complex systems clearly? |

---

## 2. The AI System Design Framework

Use this 5-phase framework for any 45-60 minute system design interview:

```
Phase 1: Clarify (5 min)     â†’ Ask questions, define scope
Phase 2: High-Level (10 min) â†’ Draw the architecture, name components
Phase 3: Deep Dive (20 min)  â†’ Detail 2-3 key components
Phase 4: Operations (10 min) â†’ Monitoring, cost, scaling, failure
Phase 5: Trade-offs (5 min)  â†’ What would you change? Risks?
```

### Phase 1: Clarifying Questions to Always Ask

**For RAG Systems:**
- How many documents? What formats? How often updated?
- Who are the users? How many concurrent users?
- Accuracy vs latency â€” which matters more?
- Do we need source citations? Multi-turn conversations?
- Data sensitivity â€” PII, compliance, access controls?

**For LLM Applications:**
- What models are we using? Budget for API costs?
- Real-time vs batch? Latency requirements?
- Do we need to support multiple models? Open-source?
- What are the safety/guardrail requirements?
- How do we measure success? What metrics?

**For ML Serving:**
- QPS (queries per second)? P99 latency target?
- Model size? GPU requirements?
- How often do models update?
- A/B testing requirements?
- Batch vs real-time or both?

### Phase 2: High-Level Architecture Template

Always start with this skeleton and adapt:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚â”€â”€â”€â–¶â”‚ API GW / â”‚â”€â”€â”€â–¶â”‚  Application â”‚â”€â”€â”€â–¶â”‚  Data    â”‚
â”‚  (Web/   â”‚    â”‚ Load     â”‚    â”‚  Layer       â”‚    â”‚  Layer   â”‚
â”‚  Mobile) â”‚    â”‚ Balancer â”‚    â”‚              â”‚    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚  AI/ML       â”‚
                                â”‚  Services    â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                â”‚  Monitoring  â”‚
                                â”‚  & Eval      â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Designing a Production RAG System

### Full Architecture

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚           INGESTION PIPELINE            â”‚
                        â”‚                                         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚Documents â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  â”‚ Parser  â”‚â”€â–¶â”‚ Chunker  â”‚â”€â–¶â”‚Embedderâ”‚ â”‚
  â”‚(PDF,HTML,â”‚         â”‚  â”‚(Unstr.) â”‚  â”‚          â”‚  â”‚        â”‚ â”‚
  â”‚ Slack,..)â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                                   â”‚     â”‚
                        â”‚                                   â–¼     â”‚
                        â”‚                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                        â”‚                            â”‚ Vector   â”‚ â”‚
                        â”‚                            â”‚ DB       â”‚ â”‚
                        â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚            QUERY PIPELINE               â”‚
                        â”‚                                         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚  User    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  â”‚ Query   â”‚â”€â–¶â”‚Retriever â”‚â”€â–¶â”‚Rerankerâ”‚ â”‚
  â”‚  Query   â”‚         â”‚  â”‚Processorâ”‚  â”‚          â”‚  â”‚        â”‚ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â”‚
                        â”‚                                   â”‚     â”‚
                        â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚     â”‚
                        â”‚       â”‚ Semantic  â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚       â”‚ Cache     â”‚                      â”‚
  â”‚ Response â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                      â”‚
  â”‚ + Sourcesâ”‚         â”‚             â–¼                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
                        â”‚       â”‚   LLM    â”‚                       â”‚
                        â”‚       â”‚Generator â”‚                       â”‚
                        â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚         EVALUATION PIPELINE             â”‚
                        â”‚                                         â”‚
                        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                        â”‚  â”‚Retrievalâ”‚  â”‚Generationâ”‚  â”‚  Eval  â”‚ â”‚
                        â”‚  â”‚ Metrics â”‚  â”‚ Metrics  â”‚  â”‚Dashboardâ”‚ â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.1 Document Processing Pipeline

**Step 1: Document Parsing**

| Format | Tool | Notes |
|--------|------|-------|
| PDF | `unstructured`, `PyMuPDF` | Handle scanned PDFs with OCR |
| HTML | `BeautifulSoup`, `trafilatura` | Strip boilerplate, keep structure |
| Markdown | `markdownify` | Preserve headers for metadata |
| DOCX | `python-docx`, `unstructured` | Extract tables separately |
| Slack/API | Custom connectors | Incremental sync with webhooks |

**Technology choice:** `unstructured.io` â€” handles 25+ file types, extracts tables, maintains document structure. Use their API ($0.01/page) or self-host.

**Step 2: Chunking**

```python
# Recursive Character Splitting (most common, good default)
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,        # tokens, not characters
    chunk_overlap=50,      # 10% overlap prevents losing context at boundaries
    separators=["\n\n", "\n", ". ", " "]  # split on paragraph > line > sentence
)
```

| Strategy | Chunk Size | Best For | Weakness |
|----------|-----------|----------|----------|
| Fixed-size (512 tokens) | 512 | General purpose | Splits mid-sentence |
| Recursive character | 256-1024 | Structured text | Requires tuning |
| Semantic (embedding-based) | Variable | Research docs | 3-5x slower, higher cost |
| Sentence-based | 3-5 sentences | Q&A systems | Too small for complex topics |
| Document-structure | Varies | Technical docs with headers | Complex implementation |

**Recommended defaults:**
- **General RAG**: Recursive character, 512 tokens, 50 token overlap
- **Legal/Medical**: Semantic chunking, respect section boundaries
- **Code**: AST-based chunking (split on functions/classes)
- **Q&A**: Sentence-based, 3-5 sentences per chunk

**Step 3: Metadata Extraction**

Always attach metadata to chunks â€” it dramatically improves retrieval:

```python
chunk_metadata = {
    "source": "confluence/engineering/deploy-guide",
    "title": "Deployment Guide v2.3",
    "section": "Rolling Updates",
    "author": "jane@company.com",
    "last_updated": "2026-01-15",
    "doc_type": "technical_guide",
    "access_level": "engineering",    # For ACL-aware RAG
    "chunk_index": 3,                 # Position in original document
    "total_chunks": 12
}
```

### 3.2 Embedding Pipeline

| Model | Dimensions | Cost/1M tokens | MTEB Score | Latency |
|-------|-----------|----------------|------------|---------|
| OpenAI text-embedding-3-small | 1536 | $0.02 | 62.3 | ~50ms |
| OpenAI text-embedding-3-large | 3072 | $0.13 | 64.6 | ~80ms |
| Cohere embed-v3 | 1024 | $0.10 | 64.5 | ~60ms |
| Voyage AI voyage-3 | 1024 | $0.06 | 67.1 | ~55ms |
| BGE-large-en-v1.5 (open) | 1024 | Self-host | 63.6 | ~30ms* |
| all-MiniLM-L6-v2 (open) | 384 | Self-host | 56.3 | ~10ms* |

*Self-hosted latency on GPU.

**Decision guide:**
- **Budget-sensitive**: `text-embedding-3-small` â€” best price/performance
- **Quality-first**: `voyage-3` or `text-embedding-3-large`
- **On-premise/privacy**: `BGE-large-en-v1.5` â€” best open-source
- **Low-latency**: `all-MiniLM-L6-v2` â€” fastest, good enough for many use cases

**Batch vs Real-time:**
- **Ingestion**: Always batch. Process 100-1000 chunks per API call
- **Queries**: Real-time. Single embedding per query (~50ms)
- **Tip**: Use async batching for ingestion to maximize throughput

### 3.3 Vector Database Selection

| Feature | Pinecone | Weaviate | Qdrant | Milvus | pgvector | ChromaDB |
|---------|----------|----------|--------|--------|----------|----------|
| **Hosting** | Managed only | Both | Both | Both | Self-host* | Self-host |
| **Max Vectors** | Billions | Billions | Billions | Billions | Millions | Millions |
| **Hybrid Search** | Yes | Yes | Yes | Yes | Limited | No |
| **Filtering** | Excellent | Excellent | Excellent | Good | SQL-native | Basic |
| **Latency (P99)** | <50ms | <100ms | <50ms | <100ms | <200ms | <100ms |
| **Cost (1M vectors)** | ~$70/mo | ~$25/mo | ~$25/mo | Free* | Free* | Free |
| **Production Ready** | Yes | Yes | Yes | Yes | Yes | No** |

*Self-hosted costs depend on infrastructure. **ChromaDB is for prototyping, not production.

**Decision guide:**
- **Startup/MVP**: ChromaDB (prototype) â†’ Pinecone or Qdrant (production)
- **Enterprise (managed)**: Pinecone â€” zero ops, excellent filtering
- **Enterprise (self-hosted)**: Qdrant or Weaviate â€” best self-hosted options
- **Already using Postgres**: pgvector â€” avoid new infrastructure for <1M vectors
- **Need hybrid search**: Weaviate or Pinecone â€” native BM25 + vector

### 3.4 Retrieval Strategies

**Level 1: Naive Vector Search (Baseline)**
```
Query â†’ Embed â†’ Top-K nearest neighbors â†’ LLM
```
- Latency: ~100ms | Quality: Baseline | Cost: Lowest
- When to use: Simple use cases, internal tools, prototyping

**Level 2: Hybrid Search (Vector + BM25)**
```
Query â†’ [Vector Search] + [BM25 Keyword Search] â†’ Reciprocal Rank Fusion â†’ LLM
```
- Latency: ~150ms | Quality: 15-25% better than naive | Cost: Low
- When to use: When exact keyword matches matter (product names, codes, IDs)

**Level 3: Hybrid + Re-ranking**
```
Query â†’ [Hybrid Search (top-50)] â†’ Re-ranker (top-5) â†’ LLM
```
- Latency: ~300ms | Quality: 20-40% better than naive | Cost: Medium
- When to use: When accuracy matters more than latency
- Re-ranker options: Cohere Rerank ($1/1K queries), cross-encoder (self-hosted)

**Level 4: Advanced (Query Expansion + Re-ranking)**
```
Query â†’ Query Decomposition â†’ [Multiple Searches] â†’ Merge â†’ Re-rank â†’ LLM
```
- Latency: ~500ms-1s | Quality: Best | Cost: Highest
- When to use: Complex questions, research use cases

**Level 5: Agentic RAG**
```
Query â†’ Agent decides strategy â†’ [Search / SQL / API / Calculator] â†’ Synthesize â†’ LLM
```
- Latency: 2-10s | Quality: Best for complex queries | Cost: Highest
- When to use: Multi-hop questions, queries needing multiple data sources

### 3.5 Generation Layer

**Prompt Template (Production RAG):**

```
You are a helpful assistant that answers questions based on the provided context.

RULES:
1. Only use information from the provided context
2. If the context doesn't contain the answer, say "I don't have enough information"
3. Always cite your sources using [Source: document_name]
4. Be concise and direct

CONTEXT:
{retrieved_chunks_with_metadata}

CONVERSATION HISTORY:
{last_3_turns}

USER QUESTION: {query}

ANSWER:
```

**Context Window Management:**
- Reserve 30% for system prompt + instructions
- Reserve 20% for conversation history
- Use remaining 50% for retrieved context
- Example: GPT-4o (128K) â†’ ~64K tokens for context â†’ ~40-60 chunks of 512 tokens

**Streaming:**
- Always stream responses for UX (first token in <500ms)
- Use Server-Sent Events (SSE) for web clients
- Buffer citations until full response is generated

### 3.6 Evaluation Pipeline

| Metric | What It Measures | Target | Tool |
|--------|-----------------|--------|------|
| **Context Precision** | Are retrieved docs relevant? | >0.8 | RAGAS |
| **Context Recall** | Did we find all relevant docs? | >0.7 | RAGAS |
| **Faithfulness** | Is the answer grounded in context? | >0.9 | RAGAS, DeepEval |
| **Answer Relevance** | Does the answer address the question? | >0.8 | RAGAS |
| **MRR (Mean Reciprocal Rank)** | Is the best doc ranked first? | >0.7 | Custom |
| **NDCG@5** | Overall ranking quality | >0.6 | Custom |
| **Hallucination Rate** | % of unsupported claims | <5% | DeepEval |
| **Latency (P99)** | End-to-end response time | <3s | Custom |

**Automated Eval Pipeline:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Test     â”‚â”€â”€â”€â–¶â”‚ RAG Pipeline â”‚â”€â”€â”€â–¶â”‚ Eval     â”‚â”€â”€â”€â–¶â”‚Dashboard â”‚
â”‚ Dataset  â”‚    â”‚ (Query +     â”‚    â”‚ Metrics  â”‚    â”‚(Grafana/ â”‚
â”‚ (Q&A     â”‚    â”‚  Retrieve +  â”‚    â”‚(RAGAS/   â”‚    â”‚ W&B)     â”‚
â”‚  pairs)  â”‚    â”‚  Generate)   â”‚    â”‚ DeepEval)â”‚    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.7 Caching Layer

**Exact Cache:** Hash the query â†’ cache the response. Hit rate: 10-20%.

**Semantic Cache:** Embed the query â†’ find similar cached queries. Hit rate: 30-60%.

```
Query â†’ Embed â†’ Search Cache (cosine sim > 0.95) â†’ Hit? Return cached response
                                                    â†’ Miss? Run full pipeline, cache result
```

- **GPTCache**: Open-source semantic caching library
- **Savings**: At 40% hit rate with GPT-4o, saves ~$400/month per 100K queries

### 3.8 Complete RAG System â€” Technology Stack

```
Ingestion:    Unstructured.io â†’ LangChain Splitter â†’ OpenAI Embeddings â†’ Qdrant
Query:        FastAPI â†’ Hybrid Search (Qdrant) â†’ Cohere Rerank â†’ GPT-4o (streaming)
Evaluation:   RAGAS + DeepEval â†’ Weights & Biases
Cache:        Redis (exact) + GPTCache (semantic)
Monitoring:   LangSmith (LLM traces) + Prometheus + Grafana
Infra:        Kubernetes + Docker + Terraform
CI/CD:        GitHub Actions â†’ eval gate â†’ deploy
```

---

## 4. Designing an LLM Application Platform

### Full Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        API GATEWAY                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Rate       â”‚  â”‚ Auth       â”‚  â”‚ Input      â”‚  â”‚ Request    â”‚  â”‚
â”‚  â”‚ Limiter    â”‚  â”‚ (JWT/API   â”‚  â”‚ Validation â”‚  â”‚ Router     â”‚  â”‚
â”‚  â”‚            â”‚  â”‚  Key)      â”‚  â”‚            â”‚  â”‚            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     APPLICATION LAYER                              â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Prompt     â”‚  â”‚ Memory     â”‚  â”‚ Agent      â”‚  â”‚ Tool       â”‚  â”‚
â”‚  â”‚ Manager    â”‚  â”‚ Manager    â”‚  â”‚ Orchestratorâ”‚  â”‚ Registry   â”‚  â”‚
â”‚  â”‚ (versions, â”‚  â”‚ (history,  â”‚  â”‚ (ReAct,    â”‚  â”‚ (search,   â”‚  â”‚
â”‚  â”‚  templates)â”‚  â”‚  summary)  â”‚  â”‚  plan+exec)â”‚  â”‚  code, DB) â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MODEL ROUTER                                   â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Complexity Classifier â†’ Route to appropriate model          â”‚  â”‚
â”‚  â”‚                                                              â”‚  â”‚
â”‚  â”‚  Simple (FAQ, greetings)  â†’ GPT-4o-mini / Haiku ($0.25/1M)  â”‚  â”‚
â”‚  â”‚  Medium (analysis, Q&A)   â†’ GPT-4o / Sonnet ($3/1M)         â”‚  â”‚
â”‚  â”‚  Complex (reasoning)      â†’ Claude Opus / o1 ($15/1M)       â”‚  â”‚
â”‚  â”‚  Private/On-prem           â†’ Llama 3.1 70B (self-hosted)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GUARDRAILS LAYER                               â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PII        â”‚  â”‚ Prompt     â”‚  â”‚ Output     â”‚  â”‚ Content    â”‚  â”‚
â”‚  â”‚ Detection  â”‚  â”‚ Injection  â”‚  â”‚ Filtering  â”‚  â”‚ Policy     â”‚  â”‚
â”‚  â”‚ (Presidio) â”‚  â”‚ Defense    â”‚  â”‚ (toxicity) â”‚  â”‚ Check      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   OBSERVABILITY                                    â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ LLM        â”‚  â”‚ Cost       â”‚  â”‚ Quality    â”‚  â”‚ Usage      â”‚  â”‚
â”‚  â”‚ Traces     â”‚  â”‚ Tracking   â”‚  â”‚ Metrics    â”‚  â”‚ Analytics  â”‚  â”‚
â”‚  â”‚(LangSmith) â”‚  â”‚ (per-call) â”‚  â”‚(eval scoresâ”‚  â”‚(per tenant)â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.1 Multi-Model Routing

**Why route?** GPT-4o costs 60x more than GPT-4o-mini. Most queries don't need the expensive model.

**Routing strategies:**

1. **Keyword-based** (simplest): If query contains "summarize" â†’ cheap model
2. **Classifier-based**: Train a small classifier on query complexity â†’ route
3. **Cascade**: Try cheap model first â†’ if confidence low, escalate to expensive model
4. **Semantic**: Embed query â†’ cluster â†’ route by cluster

**Cost savings example:**
- 100K queries/day
- Without routing: 100K Ã— GPT-4o ($3/1M input) = ~$300/day
- With routing (70% simple, 25% medium, 5% complex):
  - 70K Ã— mini ($0.15/1M) = $10.50
  - 25K Ã— GPT-4o ($3/1M) = $75
  - 5K Ã— Opus ($15/1M) = $75
  - **Total: $160.50/day (46% savings)**

### 4.2 Agent Architectures

**ReAct (Reasoning + Acting):**
```
Think â†’ Act â†’ Observe â†’ Think â†’ Act â†’ Observe â†’ ... â†’ Answer
```
- Best for: Single-step tool use, Q&A with search
- Tools: LangChain ReAct agent, Claude tool use

**Plan-and-Execute:**
```
Plan (break into steps) â†’ Execute Step 1 â†’ Execute Step 2 â†’ ... â†’ Synthesize
```
- Best for: Multi-step tasks, research, analysis
- Tools: LangGraph, CrewAI

**Multi-Agent:**
```
Orchestrator â†’ [Researcher Agent] + [Analyst Agent] + [Writer Agent] â†’ Combine
```
- Best for: Complex workflows, different expertise needed
- Tools: AutoGen, CrewAI, LangGraph

### 4.3 Memory Management

| Strategy | Max History | Latency | Cost | Best For |
|----------|------------|---------|------|----------|
| Full history | All turns | High | High | Short conversations |
| Sliding window (last N) | Last 10 | Low | Low | Chat interfaces |
| Summarization | Unlimited* | Medium | Medium | Long conversations |
| Vector memory | Unlimited* | Medium | Medium | Knowledge-heavy chats |

**Production approach**: Sliding window (last 5 turns) + summarized older history + vector memory for key facts.

### 4.4 Guardrails and Safety

```
INPUT GUARDRAILS:
  User Input â†’ PII Detection (Presidio) â†’ Prompt Injection Check â†’ Topic Filter
                    â†“                           â†“                       â†“
               Mask/Reject              Reject/Rephrase          Block/Redirect

OUTPUT GUARDRAILS:
  LLM Output â†’ Hallucination Check â†’ Toxicity Filter â†’ PII Scan â†’ Response
                     â†“                      â†“               â†“
                Flag/Retry              Filter/Edit     Mask/Remove
```

**Key tools:**
- **PII Detection**: Microsoft Presidio (open-source), AWS Comprehend
- **Prompt Injection**: Rebuff, custom classifiers, input/output sandwich
- **Toxicity**: Perspective API (Google), custom classifiers
- **Guardrails framework**: Guardrails AI, NeMo Guardrails (NVIDIA)

### 4.5 Cost Optimization

| Strategy | Savings | Implementation Effort |
|----------|---------|----------------------|
| Semantic caching | 30-60% | Medium |
| Model routing | 40-60% | Medium |
| Prompt optimization (fewer tokens) | 10-30% | Low |
| Batch processing (non-real-time) | 20-40% | Low |
| Open-source for simple tasks | 50-80% | High |
| Token budgeting (max_tokens) | 5-15% | Low |

### 4.6 Streaming and Real-Time UX

```
Client (SSE) â† API Gateway â† LLM Provider (streaming)

Timeline:
  0ms      â†’ Request sent
  200ms    â†’ First token received (TTFT - Time to First Token)
  200-3000ms â†’ Tokens stream in (~50 tokens/sec for GPT-4o)
  3000ms   â†’ Response complete
  3100ms   â†’ Citations/sources appended
```

**Implementation:**
```python
# FastAPI streaming endpoint
from fastapi.responses import StreamingResponse

@app.post("/chat")
async def chat(request: ChatRequest):
    async def generate():
        async for chunk in llm.astream(request.messages):
            yield f"data: {json.dumps({'content': chunk})}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

---

## 5. Designing ML Model Serving Infrastructure

### Full Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL SERVING PLATFORM                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Request  â”‚    â”‚ Model        â”‚    â”‚ Inference Servers       â”‚ â”‚
â”‚  â”‚ Queue    â”‚â”€â”€â”€â–¶â”‚ Router       â”‚â”€â”€â”€â–¶â”‚                        â”‚ â”‚
â”‚  â”‚ (SQS/   â”‚    â”‚ (A/B test,   â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  Redis)  â”‚    â”‚  canary,     â”‚    â”‚ â”‚Model A â”‚ â”‚Model B â”‚ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  shadow)     â”‚    â”‚ â”‚(prod)  â”‚ â”‚(canary)â”‚ â”‚ â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚ 90%    â”‚ â”‚ 10%   â”‚ â”‚ â”‚
â”‚                                      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    SUPPORTING SERVICES                    â”‚   â”‚
â”‚  â”‚                                                           â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚Feature â”‚  â”‚Model   â”‚  â”‚Auto    â”‚  â”‚ Monitoring     â”‚ â”‚   â”‚
â”‚  â”‚  â”‚Store   â”‚  â”‚Registryâ”‚  â”‚Scaler  â”‚  â”‚ (latency, GPU, â”‚ â”‚   â”‚
â”‚  â”‚  â”‚(Feast) â”‚  â”‚(MLflow)â”‚  â”‚(KEDA)  â”‚  â”‚  drift, errors)â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.1 Batch vs Real-Time Inference

| Aspect | Real-Time | Batch |
|--------|-----------|-------|
| Latency | <100ms-2s | Minutes to hours |
| Throughput | 100s-1000s QPS | Millions per run |
| Cost | Higher (always-on) | Lower (spot instances) |
| Use Cases | Chatbots, search, recommendations | Reports, email campaigns, scoring |
| Infrastructure | K8s + GPU, autoscaling | Spark, Ray, batch jobs |
| Scaling | HPA on request queue depth | More workers |

**Hybrid approach**: Pre-compute batch predictions, serve from cache, fall back to real-time for cache misses.

### 5.2 Model Registry and Versioning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL REGISTRY (MLflow)                   â”‚
â”‚                                                              â”‚
â”‚  Model: document_classifier                                  â”‚
â”‚  â”œâ”€â”€ v1.0 (Production)  - accuracy: 0.94, latency: 45ms    â”‚
â”‚  â”œâ”€â”€ v1.1 (Staging)     - accuracy: 0.96, latency: 42ms    â”‚
â”‚  â””â”€â”€ v1.2 (Development) - accuracy: 0.95, latency: 38ms    â”‚
â”‚                                                              â”‚
â”‚  Each version stores:                                        â”‚
â”‚  - Model artifacts (weights, config)                         â”‚
â”‚  - Training metrics                                          â”‚
â”‚  - Training data version (DVC hash)                          â”‚
â”‚  - Environment (requirements.txt, Docker image)              â”‚
â”‚  - Evaluation results on test set                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.3 A/B Testing and Canary for Models

```
Traffic Split:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Request Router                  â”‚
  â”‚                                â”‚
  â”‚ Model v1.0 (Control)  â”€â”€ 80%  â”‚â”€â”€â–¶ Log predictions + outcomes
  â”‚ Model v1.1 (Treatment) â”€â”€ 15% â”‚â”€â”€â–¶ Log predictions + outcomes
  â”‚ Model v1.2 (Shadow)    â”€â”€ 5%  â”‚â”€â”€â–¶ Log predictions only (no serve)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Statistical Analysis            â”‚
  â”‚                                â”‚
  â”‚ - Conversion rate comparison   â”‚
  â”‚ - Latency comparison           â”‚
  â”‚ - Error rate comparison        â”‚
  â”‚ - Statistical significance     â”‚
  â”‚ - Auto-promote if p < 0.05    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.4 Model Serving Frameworks

| Framework | GPU Support | Batching | Quantization | Streaming | Best For |
|-----------|-----------|----------|-------------|-----------|----------|
| vLLM | Excellent | PagedAttention | GPTQ, AWQ | Yes | LLM serving |
| TGI (HuggingFace) | Excellent | Continuous | GPTQ, bitsandbytes | Yes | LLM serving |
| Triton (NVIDIA) | Excellent | Dynamic | TensorRT | Yes | Multi-model |
| TorchServe | Good | Yes | Yes | No | PyTorch models |
| BentoML | Good | Adaptive | Yes | Yes | Easy deployment |
| Ray Serve | Good | Yes | Yes | Yes | Scaling |

**For LLM serving**: vLLM â€” 2-4x higher throughput than naive HuggingFace via PagedAttention.

**For traditional ML**: BentoML or Ray Serve â€” easy to deploy scikit-learn, XGBoost, PyTorch models.

### 5.5 Feature Stores

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FEATURE STORE (Feast)               â”‚
â”‚                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ OFFLINE STORE    â”‚    â”‚ ONLINE STORE             â”‚  â”‚
â”‚  â”‚ (S3/BigQuery)    â”‚    â”‚ (Redis/DynamoDB)         â”‚  â”‚
â”‚  â”‚                  â”‚    â”‚                          â”‚  â”‚
â”‚  â”‚ - Training data  â”‚    â”‚ - Serving features       â”‚  â”‚
â”‚  â”‚ - Batch features â”‚â”€â”€â”€â–¶â”‚ - Low-latency lookup     â”‚  â”‚
â”‚  â”‚ - Historical     â”‚    â”‚ - Latest feature values  â”‚  â”‚
â”‚  â”‚ - TB-scale       â”‚    â”‚ - <10ms P99              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                       â”‚
â”‚  Feature definitions:                                 â”‚
â”‚  - user_purchase_count_7d (computed daily)            â”‚
â”‚  - user_avg_session_duration (computed hourly)        â”‚
â”‚  - item_view_count_24h (computed hourly)              â”‚
â”‚  - user_embedding_v2 (computed weekly)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Designing an MLOps Pipeline

### Full Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MLOPS PIPELINE                               â”‚
â”‚                                                                    â”‚
â”‚  DATA LAYER                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Data    â”‚â”€â–¶â”‚ Data     â”‚â”€â–¶â”‚ Feature  â”‚â”€â–¶â”‚ Data Versioning  â”‚  â”‚
â”‚  â”‚ Sources â”‚  â”‚ Validationâ”‚  â”‚ Engineer â”‚  â”‚ (DVC)            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚(Great    â”‚  â”‚(Feast/   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚               â”‚Expectat.)â”‚  â”‚ custom)  â”‚                         â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                   â”‚                               â”‚
â”‚  TRAINING LAYER                   â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Experimentâ”‚â”€â–¶â”‚ Training â”‚â”€â–¶â”‚ Model    â”‚â”€â–¶â”‚ Model Registry   â”‚ â”‚
â”‚  â”‚ Tracking â”‚  â”‚ Pipeline â”‚  â”‚ Evaluationâ”‚  â”‚ (MLflow)         â”‚ â”‚
â”‚  â”‚ (W&B/   â”‚  â”‚ (Kubeflowâ”‚  â”‚ (test setâ”‚  â”‚                  â”‚ â”‚
â”‚  â”‚  MLflow) â”‚  â”‚  /Vertex)â”‚  â”‚  + eval) â”‚  â”‚ v1.0 â†’ v1.1     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                    â”‚              â”‚
â”‚  DEPLOYMENT LAYER                                  â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ CI/CD    â”‚â”€â–¶â”‚ Model    â”‚â”€â–¶â”‚ Canary / â”‚â”€â–¶â”‚ Production       â”‚ â”‚
â”‚  â”‚ Pipeline â”‚  â”‚ Validationâ”‚  â”‚ A/B Test â”‚  â”‚ Serving          â”‚ â”‚
â”‚  â”‚(GitHub   â”‚  â”‚ Gate     â”‚  â”‚          â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚ Actions) â”‚  â”‚          â”‚  â”‚          â”‚  â”‚                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                    â”‚              â”‚
â”‚  MONITORING LAYER                                  â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Data     â”‚  â”‚ Model    â”‚  â”‚ Infra    â”‚  â”‚ Alerting         â”‚ â”‚
â”‚  â”‚ Drift    â”‚  â”‚ Perf     â”‚  â”‚ Metrics  â”‚  â”‚ (PagerDuty)      â”‚ â”‚
â”‚  â”‚ Monitor  â”‚  â”‚ Monitor  â”‚  â”‚ (GPU,    â”‚  â”‚                  â”‚ â”‚
â”‚  â”‚(Evidentlyâ”‚  â”‚          â”‚  â”‚  latency)â”‚  â”‚ Drift > thresholdâ”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.1 Training Pipeline

**Data Versioning (DVC):**
```
project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          â† tracked by DVC (not Git)
â”‚   â”œâ”€â”€ processed/    â† tracked by DVC
â”‚   â””â”€â”€ data.dvc      â† tracked by Git (pointer file)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ model.pkl.dvc â† tracked by Git
â”œâ”€â”€ dvc.yaml          â† pipeline definition
â””â”€â”€ params.yaml       â† hyperparameters
```

**Experiment Tracking (Weights & Biases):**
```python
import wandb

wandb.init(project="document-classifier", config={
    "model": "bert-base",
    "learning_rate": 2e-5,
    "epochs": 10,
    "batch_size": 32
})

# During training
wandb.log({"train_loss": loss, "val_accuracy": acc, "epoch": epoch})

# After training
wandb.log({"test_accuracy": 0.96, "test_f1": 0.94})
wandb.save("model.pt")
```

### 6.2 CI/CD for ML

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ML CI/CD PIPELINE                           â”‚
â”‚                                                              â”‚
â”‚  PR Opened                                                   â”‚
â”‚    â”œâ”€â”€ Run unit tests                                        â”‚
â”‚    â”œâ”€â”€ Run data validation (Great Expectations)              â”‚
â”‚    â”œâ”€â”€ Run model training on sample data                     â”‚
â”‚    â””â”€â”€ Run model evaluation on test set                      â”‚
â”‚                                                              â”‚
â”‚  PR Merged to main                                           â”‚
â”‚    â”œâ”€â”€ Full training on production data                      â”‚
â”‚    â”œâ”€â”€ Model evaluation + comparison to current production   â”‚
â”‚    â”œâ”€â”€ PERFORMANCE GATE:                                     â”‚
â”‚    â”‚   â”œâ”€â”€ accuracy >= current_prod - 0.01? âœ…               â”‚
â”‚    â”‚   â”œâ”€â”€ latency <= current_prod * 1.1?   âœ…               â”‚
â”‚    â”‚   â””â”€â”€ no data quality issues?           âœ…               â”‚
â”‚    â”œâ”€â”€ Register model in MLflow (Staging)                    â”‚
â”‚    â”œâ”€â”€ Deploy canary (10% traffic)                           â”‚
â”‚    â”œâ”€â”€ Monitor for 2 hours                                   â”‚
â”‚    â””â”€â”€ Promote to Production (100% traffic) or Rollback     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.3 Monitoring for ML

**Data Drift Detection:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Drift Monitor (Evidently AI)                      â”‚
â”‚                                                    â”‚
â”‚ Feature: user_age                                  â”‚
â”‚ â”œâ”€â”€ Training distribution: mean=35, std=12         â”‚
â”‚ â”œâ”€â”€ Production (today):    mean=42, std=8          â”‚
â”‚ â”œâ”€â”€ PSI (Population Stability Index): 0.18         â”‚
â”‚ â””â”€â”€ Status: âš ï¸ WARNING (PSI > 0.1)                â”‚
â”‚                                                    â”‚
â”‚ Feature: text_length                               â”‚
â”‚ â”œâ”€â”€ Training distribution: mean=150, std=80        â”‚
â”‚ â”œâ”€â”€ Production (today):    mean=145, std=75        â”‚
â”‚ â”œâ”€â”€ PSI: 0.02                                      â”‚
â”‚ â””â”€â”€ Status: âœ… STABLE                              â”‚
â”‚                                                    â”‚
â”‚ Actions:                                           â”‚
â”‚ - PSI > 0.1: Alert team                            â”‚
â”‚ - PSI > 0.25: Trigger retraining pipeline          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Model Performance Monitoring:**

| Metric | Check Frequency | Alert Threshold |
|--------|----------------|-----------------|
| Prediction accuracy | Hourly (sample) | Drop > 5% from baseline |
| Latency P99 | Real-time | > 2x baseline |
| Error rate | Real-time | > 1% |
| Data drift (PSI) | Daily | PSI > 0.1 |
| Concept drift | Weekly | Accuracy drop > 3% |
| Feature availability | Real-time | Any feature missing |

---

## 7. Production Considerations

### 7.1 Cost Management

**LLM API Cost Estimation (per 100K queries/day):**

| Model | Input Cost | Output Cost | Total/Day | Total/Month |
|-------|-----------|-------------|-----------|-------------|
| GPT-4o-mini | $1.50 | $6.00 | $7.50 | $225 |
| GPT-4o | $25.00 | $100.00 | $125.00 | $3,750 |
| Claude Sonnet 4.5 | $30.00 | $150.00 | $180.00 | $5,400 |
| Claude Haiku | $8.00 | $40.00 | $48.00 | $1,440 |
| Llama 3.1 70B (self-hosted) | ~$50 GPU/day | â€” | $50 | $1,500 |

*Assumes avg 500 input tokens + 200 output tokens per query.*

**Embedding Cost Estimation (per 1M documents):**

| Component | Cost |
|-----------|------|
| Embedding (text-embedding-3-small) | ~$4 |
| Vector DB storage (Pinecone, 1M vectors) | ~$70/mo |
| Re-ranking (Cohere, 1M queries) | ~$1,000/mo |

### 7.2 Latency Budgets

| Component | Target P50 | Target P99 | Optimization |
|-----------|-----------|-----------|-------------|
| Embedding (query) | 30ms | 80ms | Batch, cache |
| Vector search | 10ms | 50ms | Index tuning, replicas |
| Re-ranking | 50ms | 150ms | Top-K limit, smaller model |
| LLM generation (streaming) | 200ms TTFT | 500ms TTFT | Model routing, cache |
| **Total RAG pipeline** | **300ms TTFT** | **800ms TTFT** | â€” |
| LLM generation (full) | 1.5s | 4s | Smaller model, max_tokens |
| **Total RAG (non-streaming)** | **2s** | **5s** | â€” |

### 7.3 Scaling Patterns

**Horizontal scaling (most common):**
- Multiple API server replicas behind load balancer
- Scale on request queue depth or latency
- Each replica is stateless (state in Redis/DB)

**GPU scaling:**
- GPU instances are expensive ($2-8/hour)
- Use spot/preemptible instances for batch inference (70% savings)
- Right-size GPU: Don't use A100 for a small model
- Quantization: INT8 reduces memory 2x, INT4 reduces 4x with ~1% quality loss

**Queue-based scaling:**
- Put inference requests in a queue (SQS, RabbitMQ)
- Workers pull from queue, scale workers with KEDA
- Good for: batch processing, handling traffic spikes

### 7.4 Observability for AI

**LLM-Specific Metrics to Track:**

| Metric | Why | Tool |
|--------|-----|------|
| Token usage (input/output) | Cost control | LangSmith, custom |
| Latency per model call | Performance | LangSmith, Prometheus |
| Cache hit rate | Cost optimization | Custom metrics |
| Hallucination rate | Quality | Eval pipeline |
| User satisfaction (thumbs up/down) | Quality | Custom |
| Retrieval relevance score | RAG quality | RAGAS |
| Error rate by model | Reliability | Prometheus |
| Cost per query | Budget | Custom |

**LLM Tracing (LangSmith):**
```
Trace: user_query_12345
â”œâ”€â”€ Input: "What is our refund policy?"
â”œâ”€â”€ Retrieval
â”‚   â”œâ”€â”€ Query embedding: 45ms
â”‚   â”œâ”€â”€ Vector search: 12ms (5 results)
â”‚   â”œâ”€â”€ Re-ranking: 89ms (top 3)
â”‚   â””â”€â”€ Total retrieval: 146ms
â”œâ”€â”€ Generation
â”‚   â”œâ”€â”€ Model: gpt-4o
â”‚   â”œâ”€â”€ Input tokens: 1,234
â”‚   â”œâ”€â”€ Output tokens: 187
â”‚   â”œâ”€â”€ TTFT: 198ms
â”‚   â”œâ”€â”€ Total: 1,890ms
â”‚   â””â”€â”€ Cost: $0.0043
â”œâ”€â”€ Guardrails
â”‚   â”œâ”€â”€ PII check: 5ms (clean)
â”‚   â””â”€â”€ Toxicity: 3ms (clean)
â”œâ”€â”€ Total latency: 2,044ms
â””â”€â”€ User feedback: ğŸ‘
```

---

## 8. Common Trade-offs

### RAG vs Fine-tuning vs Prompt Engineering

| Factor | Prompt Engineering | RAG | Fine-tuning |
|--------|-------------------|-----|-------------|
| **Cost to start** | $0 | $100-500 | $500-10,000 |
| **Data needed** | 0 examples | 100+ documents | 1,000+ examples |
| **Time to deploy** | Hours | Days | Weeks |
| **Update frequency** | Instant | Minutes (re-index) | Hours-days (retrain) |
| **Accuracy on domain data** | Low-Medium | High | Highest |
| **Hallucination risk** | High | Low (grounded) | Medium |
| **Latency impact** | None | +100-500ms | None (or faster) |
| **Best for** | Prototyping, simple tasks | Knowledge-heavy apps | Style/format, classification |

**Decision flowchart:**
```
Need domain-specific knowledge?
â”œâ”€â”€ No â†’ Prompt Engineering
â””â”€â”€ Yes â†’ Knowledge changes frequently?
    â”œâ”€â”€ Yes â†’ RAG
    â””â”€â”€ No â†’ Need specific output format/style?
        â”œâ”€â”€ Yes â†’ Fine-tuning (+ RAG if also knowledge-heavy)
        â””â”€â”€ No â†’ RAG
```

### Accuracy vs Latency vs Cost Triangle

```
                    ACCURACY
                       â–²
                      / \
                     /   \
                    /     \
                   / Pick  \
                  /  Two    \
                 /           \
                â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼
            LATENCY â—€â”€â”€â”€â”€â”€â”€â”€â–¶ COST
```

**You can optimize for two, but the third suffers:**
- **High Accuracy + Low Latency** = High Cost (GPT-4o, multiple retrieval passes, re-ranking)
- **High Accuracy + Low Cost** = High Latency (batch processing, open-source models, extensive eval)
- **Low Latency + Low Cost** = Lower Accuracy (small models, no re-ranking, simple retrieval)

### Open-Source vs Proprietary Models

| Factor | Proprietary (GPT-4o, Claude) | Open-Source (Llama, Mistral) |
|--------|------------------------------|------------------------------|
| **Quality** | Best (for now) | Close for many tasks |
| **Cost at scale** | $3-15/1M tokens | $0.50-2/1M (self-hosted) |
| **Data privacy** | Data sent to provider | Full control |
| **Latency** | 200-500ms TTFT | 50-200ms TTFT (self-hosted) |
| **Reliability** | 99.9% SLA | You own uptime |
| **Customization** | Prompt only | Fine-tune, quantize |
| **Setup effort** | Minutes | Days-weeks |
| **GPU needed** | No | Yes ($2-8/hr) |

**Decision: Start with proprietary APIs. Move to open-source when:**
- Cost exceeds $5K/month
- Data privacy requirements prevent API usage
- You need <100ms latency
- You need custom model behavior that prompting can't achieve

---

## 9. Interview Questions with Answers

### Q1: "Design a Customer Support Chatbot with RAG"

**Clarifying questions:**
- How many support articles? â†’ ~10,000, updated weekly
- Daily users? â†’ ~5,000 conversations/day
- Need to handle ticket creation? â†’ Yes, escalate to human if can't resolve
- Latency requirement? â†’ First response <2 seconds
- Languages? â†’ English only for now

**High-level architecture:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User â”‚â”€â”€â”€â–¶â”‚ Chat UI  â”‚â”€â”€â”€â–¶â”‚ Chat Service (FastAPI)              â”‚
â”‚      â”‚â—€â”€â”€â”€â”‚(React/WS)â”‚â—€â”€â”€â”€â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                             â”‚  â”‚ Intent      â”‚  â”‚ RAG Pipelineâ”‚ â”‚
                             â”‚  â”‚ Classifier  â”‚â”€â”€â–¶â”‚             â”‚ â”‚
                             â”‚  â”‚(simple/faq/ â”‚  â”‚ Retrieve â†’  â”‚ â”‚
                             â”‚  â”‚ complex/    â”‚  â”‚ Rerank â†’    â”‚ â”‚
                             â”‚  â”‚ escalate)   â”‚  â”‚ Generate    â”‚ â”‚
                             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                             â”‚                                    â”‚
                             â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                             â”‚  â”‚ Ticket      â”‚  â”‚ Conversationâ”‚ â”‚
                             â”‚  â”‚ Service     â”‚  â”‚ Memory      â”‚ â”‚
                             â”‚  â”‚ (Zendesk)   â”‚  â”‚ (Redis)     â”‚ â”‚
                             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key decisions:**
- **Intent classifier** (GPT-4o-mini, $0.15/1M): Route simple FAQs to cached answers, complex to RAG, frustrated users to human
- **RAG retrieval**: Hybrid search (vector + BM25) with Cohere re-ranking
- **Escalation**: If confidence < 0.7 or user says "talk to human" â†’ create Zendesk ticket with conversation summary
- **Memory**: Redis for session state, last 5 turns in context

**Metrics to monitor:**
- Resolution rate (% resolved without human)
- CSAT score (post-chat survey)
- Escalation rate
- Average response latency
- Cost per conversation

---

### Q2: "Design a Document Search Engine for a Law Firm"

**Key requirements:** 500K legal documents, 200 lawyers, exact legal citations critical, privileged documents with strict access control.

**Architecture highlights:**
- **ACL-aware RAG**: Filter vector search by user's access permissions
- **Hybrid search**: Lawyers need exact case citations (BM25) + semantic understanding
- **Re-ranking**: Cross-encoder re-ranking â€” accuracy is critical for legal
- **Chunking**: Document-structure-aware (respect sections, paragraphs, footnotes)
- **Citation**: Every response must cite paragraph numbers, document IDs
- **Audit log**: Every query and result logged for compliance

**Trade-offs:**
- Higher latency acceptable (lawyers expect 3-5s for complex queries)
- Zero tolerance for hallucination (use faithfulness score > 0.95 gate)
- On-premise deployment for data sovereignty

---

### Q3: "Design a Real-Time Content Moderation System"

**Key requirements:** 1M posts/day, <500ms, text + images, minimize false positives.

**Architecture:**
```
Post â†’ [Fast Classifier (50ms)] â†’ Safe (95%) â†’ Publish
                                 â†’ Uncertain (4%) â†’ [LLM Review (300ms)] â†’ Safe/Block
                                 â†’ Toxic (1%) â†’ Block + Queue for Human Review
```

**Key decisions:**
- **Two-stage pipeline**: Fast classifier (DistilBERT) catches obvious cases, LLM handles edge cases
- **Why not LLM for everything?** At 1M posts/day, GPT-4o would cost ~$5K/day. Fast classifier costs ~$50/day
- **Image moderation**: CLIP or Google Vision API for image classification
- **Human-in-the-loop**: Uncertain cases go to moderation queue, human decisions feed back into training

---

### Q4: "Design an AI-Powered Code Review Tool"

**Key requirements:** Integrate with GitHub PRs, review for bugs, security, style, <30s per PR.

**Architecture highlights:**
- **Trigger**: GitHub webhook on PR creation/update
- **Context gathering**: Pull diff, full file context, repo conventions (from existing PRs)
- **Multi-pass review**:
  1. Security scan (static analysis + LLM for logic bugs)
  2. Bug detection (LLM with full function context)
  3. Style/readability (smaller model, compare to repo conventions)
- **Comment generation**: Post inline GitHub comments with suggestions
- **Learning**: Track which suggestions are accepted/rejected to improve

---

### Q5: "Design a Multi-Tenant LLM Platform"

**Key requirements:** 100 enterprise clients, each with different models, rate limits, data isolation.

**Architecture:**
```
Client A â”€â”
Client B â”€â”¼â”€â”€â–¶ API Gateway â”€â”€â–¶ Tenant Router â”€â”€â–¶ [Model Pool]
Client C â”€â”˜    (API key â†’      (config lookup)    â”œâ”€â”€ OpenAI
                tenant ID)                         â”œâ”€â”€ Anthropic
                                                   â”œâ”€â”€ Self-hosted Llama
                                                   â””â”€â”€ Custom fine-tuned
```

**Key decisions:**
- **Tenant isolation**: Separate API keys, rate limits, model configs per tenant
- **Data isolation**: No cross-tenant data leakage. Separate vector stores per tenant
- **Usage tracking**: Log every API call with tenant ID, model, tokens, cost
- **Billing**: Calculate cost per tenant, support different pricing tiers
- **Guardrails per tenant**: Each tenant can configure their own content policies

---

## Quick Reference Card

### The 5-Phase Framework
```
1. CLARIFY (5 min)   â†’ Users, scale, latency, accuracy, constraints
2. HIGH-LEVEL (10m)  â†’ Draw boxes and arrows, name components
3. DEEP DIVE (20m)   â†’ Detail 2-3 components with specifics
4. OPERATIONS (10m)  â†’ Monitoring, cost, scaling, failures
5. TRADE-OFFS (5m)   â†’ What would you change? Biggest risk?
```

### Numbers to Know
```
Embedding latency:     30-80ms per query
Vector search:         10-50ms per query
LLM TTFT (streaming):  200-500ms
LLM full response:     1-4 seconds
Re-ranking:            50-150ms

GPT-4o:          $2.50 input / $10 output per 1M tokens
GPT-4o-mini:     $0.15 input / $0.60 output per 1M tokens
Claude Sonnet:   $3 input / $15 output per 1M tokens
Claude Haiku:    $0.80 input / $4 output per 1M tokens
Embeddings:      $0.02-0.13 per 1M tokens

Pinecone:        ~$70/mo per 1M vectors
Qdrant:          ~$25/mo self-hosted per 1M vectors
```

### Magic Phrases for Interviews
- "Let me start by clarifying the requirements..."
- "The trade-off here is between X and Y..."
- "At this scale, we'd need to consider..."
- "For monitoring, I'd track these key metrics..."
- "If we needed to scale 10x, I'd change..."
- "The biggest risk in this design is..."
- "I'd start simple with X, then migrate to Y as we scale..."
